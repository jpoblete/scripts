Resources:

https://spark.apache.org/docs/1.4.0/sql-programming-guide.html
https://drive.google.com/a/datastax.com/file/d/0BwempBLstPAYalBQQl9Fc3hESjA/view?usp=sharing
https://github.com/datastax/spark-cassandra-connector/blob/master/doc/14_data_frames.md

Spark Exercises...

Run this on CQLSH

CREATE KEYSPACE studentdata WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'}  AND durable_writes = true;

CREATE TABLE studentdata.students (
    name text PRIMARY KEY,
    age int,
    subcodes set<text>
) WITH bloom_filter_fp_chance = 0.01
    AND caching = '{"keys":"ALL", "rows_per_partition":"NONE"}'
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy'}
    AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99.0PERCENTILE';

Add this data to the table

$ cat students.dat 
student4,22,{'ART2002'}
student6,21,{'SCI1002'}
student3,21,{'ART2001'}
student1,18,"{'SCI1001', 'SCI1002'}"
student5,23,"{'HIS3001', 'SCI1001'}"
student2,19,"{'SCI1001', 'SCI1002'}"

From CQLSH

COPY studentdata.students FROM 'students.dat';
SELECT * FROM studentdata.students;

 name     | age | subcodes
----------+-----+------------------------
 student3 |  21 |            {'ART2001'}
 student4 |  22 |            {'ART2002'}
 student6 |  21 |            {'SCI1002'}
 student5 |  23 | {'HIS3001', 'SCI1001'}
 student2 |  19 | {'SCI1001', 'SCI1002'}
 student1 |  18 | {'SCI1001', 'SCI1002'}

(6 rows)

Edit file /etc/dse/spark/logback-spark.xml and make it to look like this

<configuration scan="true">
  ...
    <logger name="org.eclipse.jetty" level="ERROR"/>
    <logger name="com.datastax.driver.core" level="DEBUG"/>
</configuration>

That will start Java driver logging - which is used by the Spark-Cassandra connector

Start the DSE Spark shell and define the Dataframe

import org.apache.spark.sql._
import org.apache.spark.sql.cassandra._
val sqlContext = new SQLContext(sc)
// Dataframe definition
val df = sqlContext.read.format("org.apache.spark.sql.cassandra").options(Map( "table" -> "students", "keyspace" -> "studentdata" )).load()
// Print the schema in a tree format
df.printSchema()
root
 |-- name: string (nullable = true)
 |-- age: integer (nullable = true)
 |-- subcodes: array (nullable = true)
 |    |-- element: string (containsNull = true)

// Displays the content of the DataFrame to stdout
df.show
+--------+---+--------------------+
|    name|age|            subcodes|
+--------+---+--------------------+
|student4| 22|ArrayBuffer(ART2002)|
|student5| 23|ArrayBuffer(HIS30...|
|student3| 21|ArrayBuffer(ART2001)|
|student6| 21|ArrayBuffer(SCI1002)|
|student2| 19|ArrayBuffer(SCI10...|
|student1| 18|ArrayBuffer(SCI10...|
+--------+---+--------------------+

// Select only the "name" column
df.select("name").show()
+--------+
|    name|
+--------+
|student4|
|student5|
|student3|
|student6|
|student2|
|student1|
+--------+

// Select people older than 21
df.filter(df("age") > 21).show()
+--------+---+--------------------+
|    name|age|            subcodes|
+--------+---+--------------------+
|student4| 22|ArrayBuffer(ART2002)|
|student5| 23|ArrayBuffer(HIS30...|
+--------+---+--------------------+

// Select the names of people older than 21
df.filter(df("age") > 21).select("name").show()
+--------+
|    name|
+--------+
|student4|
|student5|
+--------+

// Count people by age
df.groupBy("age").count().show()
+---+-----+                                                                     
|age|count|
+---+-----+
| 18|    1|
| 19|    1|
| 21|    2|
| 22|    1|
| 23|    1|
+---+-----+


Another example using HiveContext

$ cqlsh
Connected to Test Cluster at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 2.1.17.1439 | DSE 4.8.12 | CQL spec 3.2.1 | Native protocol v3]
Use HELP for help.
cqlsh> CREATE KEYSPACE t33734 WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1 } AND durable_writes = 'true';
cqlsh> use t33734;
cqlsh:t33734> CREATE TABLE students (
          ...     name text PRIMARY KEY,
          ...     age int,
          ...     subcodes set<text>
          ... );
cqlsh:t33734> CREATE TABLE ages (name text PRIMARY KEY, age int);
cqlsh:t33734> exit
$ cat students.dat 
student4,22,{'ART2002'}
student6,21,{'SCI1002'}
student3,21,{'ART2001'}
student1,18,"{'SCI1001', 'SCI1002'}"
student5,23,"{'HIS3001', 'SCI1001'}"
student2,19,"{'SCI1001', 'SCI1002'}"
$ cqlsh -k t33734 -e "COPY students FROM 'students.dat';"
Using 3 child processes

Starting copy of t33734.students with columns ['name', 'age', 'subcodes'].
Processed: 6 rows; Rate:       6 rows/s; Avg. rate:       9 rows/s
6 rows imported from 1 files in 0.660 seconds (0 skipped).
jose@Mac:~/Downloads/33734$ cqlsh -k t33734 -e "SELECT * FROM students;"

 name     | age | subcodes
----------+-----+------------------------
 student3 |  21 |            {'ART2001'}
 student4 |  22 |            {'ART2002'}
 student6 |  21 |            {'SCI1002'}
 student5 |  23 | {'HIS3001', 'SCI1001'}
 student2 |  19 | {'SCI1001', 'SCI1002'}
 student1 |  18 | {'SCI1001', 'SCI1002'}

(6 rows)
$
$ dse spark
...

scala> hc
scala> import org.apache.spark.sql._
scala> import org.apache.spark.sql.cassandra._
scala> val df = hc.read.format("org.apache.spark.sql.cassandra").options(Map( "table" -> "students", "keyspace" -> "t33734" )).load()
scala> df.filter(df("age") > 21).show()
+--------+---+--------------------+
|    name|age|            subcodes|
+--------+---+--------------------+
|student4| 22|ArrayBuffer(ART2002)|
|student5| 23|ArrayBuffer(HIS30...|
+--------+---+--------------------+

scala> df.filter(df("age") > 21).select("name", "age").show()
+--------+---+
|    name|age|
+--------+---+
|student4| 22|
|student5| 23|
+--------+---+

scala> df.filter(df("age") > 21).select("name", "age").write.format("org.apache.spark.sql.cassandra").options(Map("table" -> "ages", "keyspace" -> "t33734")).save()
scala> val df2 = hc.read.format("org.apache.spark.sql.cassandra").options(Map( "table" -> "ages", "keyspace" -> "t33734" )).load()
scala> df2.show()
+--------+---+
|    name|age|
+--------+---+
|student4| 22|
|student5| 23|
+--------+---+

scala> 



Here is an example of how to get rowcount


From CQLSH
==========

CREATE KEYSPACE t31206 WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '3'}  AND durable_writes = true;

CREATE TABLE t31206.tester (
    id int PRIMARY KEY,
    info text
)

Populate the table
==================

$ awk '{OFS=",";print NR,$0}' /usr/share/dict/words | cqlsh -e "COPY t31206.tester FROM STDIN;"
Using 3 child processes

Starting copy of t31206.tester with columns ['id', 'info'].
[Use \. on a line by itself to end input]
Processed: 235886 rows; Rate:    7868 rows/s; Avg. rate:    8077 rows/s
235886 rows imported from 1 files in 29.205 seconds (0 skipped).

Spark rowcount
==============

$ dse spark
The log file is at /Users/jose/.spark-shell.log
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.4.2
      /_/

Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_73)
Type in expressions to have them evaluated.
Type :help for more information.
Creating SparkContext...
Initializing SparkContext with MASTER: spark://127.0.0.1:7077
Created spark context..
Spark context available as sc.
WARN  2017-02-16 16:35:57,047 org.apache.hadoop.hive.cassandra.CassandraManager: Default CL is LOCAL_ONE. Because of replication factor of local data center is less than 1, set CL to ONE
Hive context available as hc.
CassandraSQLContext available as csc.

scala> hc;
res0: org.apache.spark.sql.hive.HiveContext = org.apache.spark.sql.hive.HiveContext@3f465ae

scala> val sqlContext = new org.apache.spark.sql.SQLContext(sc)
sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@8ba7408

scala> import sqlContext.implicits._
import sqlContext.implicits._

scala> import org.apache.spark.sql._ 
import org.apache.spark.sql._

scala> val df = sqlContext.read.format("org.apache.spark.sql.cassandra").options(Map("keyspace"->"t31206","table"->"tester")).load
WARN  2017-02-16 16:39:43,941 com.datastax.driver.core.NettyUtil: Found Netty's native epoll transport, but not running on linux-based operating system. Using NIO instead.
df: org.apache.spark.sql.DataFrame = [id: int, info: string]

scala> df.count()
res1: Long = 235886

scala> 



